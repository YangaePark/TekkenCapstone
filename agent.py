# Main file for the neural network
# atari network
# https://arxiv.org/pdf/1312.5602.pdf
# https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/
import os
import random
import numpy as np
from SumTree import SumTree
from keras.models import Sequential
from keras.layers import Conv2D, Dense, Flatten
from keras.optimizers import *
from keyboardCombos import KeyboardCombos
from windowsInputHandler import InputHandler

IMAGE_STACK = 2
IMAGE_WIDTH = 84
IMAGE_HEIGHT = 84
LEARNING_RATE = 0.00025


class Model:

    def __init__(self, stateCnt, actionCnt):
        self.stateCnt = stateCnt
        self.actionCnt = actionCnt

        self.model = self._createModl()
        self.target_model = self._createModel()

    def _createModel(self):
        model = Sequential()

        model.add(Conv2D(32, (8,8), strides=(4,4), activation='relu', input_shape=(self.stateCnt), data_format='channels_first'))
        model.add(Conv2D(64, (4,4), strides=(2,2), activation='relu'))
        model.add(Conv2D(64, (3,3), activation='relu'))
        model.add(Flatten())
        model.add(Dense(units=512, activation='relu'))
        model.add(Dense(units=actionCnt, activation='linear'))

        opt = RMSprop(lr=LEARNING_RATE)
        model.compile(loss=huber_loss, optimizer=opt)

        return model

    def train(self, x, y, epochs=1, verbose=0):
        self.model.fit(x, y, batch_size=32, epochs=epochs, verbose=verbose)

    def predict(self, s, target=False):
        if target:
            return self.target_model.predict(s)
        else:
            return self.model.predict(s)

    def predict_one(self, s, target=False):
        return self.predict(s.reshape(1, IMAGE_STACK, IMAGE_WIDTH, IMAGE_HEIGHT), target).flatten()

    def update_target_model(self):
        self.target_model.set_weights(self.model.get_weights())

class LearningAgent(Agent):

    def __init__(self, learning=false, epsilon=1.0, alpha=0.5):
        super(LearningAgent, self).__init__()
        self.inputHandler = KeyboardCombos(self)
        self.valid_actions = []

    def reset(self, testing=False):
        # Call me when a "round" is over
        pass

    def build_state(self):
        # The current state that the agent is in based on the environment
        pass

    def get_maxQ(self, state):
        # access the sumTree to get the max Q value for the state.
        pass

    def createQ(self, state):
        # call after the state is generated by the agent.
        pass

    def choose_action(self, state):
        # call when the agent must make a decision based on the state
        pass

    def learn(self, state,action,reward):
        # The learn function is called after the agent completes an actiona nd receives an award.
        # this function does not consider future rewards when learning
        pass

    def update(self):
        pass

# Gets screen data for HP/screen monitoring and reward feedback
import mss
from PIL import Image
class Vision:
    screen = {'top': 40, 'left': 0, 'width': 640, 'height': 350}
    leftHPCapture = {'top': 62, 'left': 84, 'width': 201, 'height': 12}
    rightHPCapture = {'top': 62, 'left': 360, 'width': 201, 'height': 12}

    positive = 1    # AI hit the opponent
    negative = -1   # AI took a hit

    def __init__(self, side):
        self.side = side
        with mss as sct():
            self.prevLeftHP = np.array(sct.grab(leftHPCapture))
            self.prevRightHP = np.array(sct.grab(rightHPCapture))

    # Dot product to reduce the pixel values to their grayscale equivlalent
    def numpyImgToGray(self, img):
        return np.dot(img[...,:3], [0.299,0.587,0.114])

    def getCurrentScreen(self):
        with mss as sct():
            sct_img = sct.grab(screen)
            img = Image.frombytes('RGB', sct_img.size, sct_img.rgb)
            img = img.resize((IMAGE_WIDTH, IMAGE_HEIGHT), Image.LANCZOS)
            currScreen = np.array(img)
            return numpyImgToGray(currScreen)

    def getReward(self):
        with mss as sct():
            reward = 0

            currLeft = np.array(sct.grab(leftHPCapture))
            currRight = np.array(sct.grab(rightHPCapture))
            # Convert to gray
            currLeft = numpyImgToGray(currLeft)
            currRight = numpyImgToGray(currRight)
            # get the difference in previous vs current
            diffLeft = self.prevLeft - currLeft
            diffRight = self.prevRight - currRight
            # round negative values up to 0
            diffLeft = diffLeft.clip(min=0)
            diffRight = diffRight.clip(min=0)
            # If hit, there are typically more than 10 pixels with values > 120
            if((diffLeft > 125).sum() > 10):
                if(side == 'left'):
                    reward = reward - 1
                else:
                    reward = reward + 1
            # If hit, there are typically more than 10 pixels with values > 120
            if((diffRight > 125).sum() > 10):
                if(side == 'right'):
                    reward = reward - 1
                else:
                    reward = reward + 1
            # Set previous frame data to current frame data
            self.prevLeft = currLeft
            self.prevRight = currRight

            return reward

# SumTree of previous decisions
class Memory:
    def __init__(self, capacity, epsilon=1.0, alpha=0.5):
        self.tree = SumTree(capacity)
        self.epsilon = epsilon
        self.alpha = alpha

    def _getPriority(self, error):
        return (error + self.epsilon) ** self.alpha

    def add(self, error, sample):
        p = self._getPriority(error)
        self.tree.add(p, sample)

    def sample(self, n):
        batch = []
        segment = self.tree.total() / n

        for i in range(n):
            a = segment * i
            b = segment * (i + 1)

            s = random.uniform(a, b)
            (idx, p, data) = self.tree.get(s)
            batch.append((idx, data))

        return batch

    def update(self, idx, error):
        p = self._getPriority(error)
        self.tree.update(idx, p)

# lets start the show
def run():

if __name__ == '__main__':
    run()
